# -*- coding: utf-8 -*-
"""Copy of ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1biDk7nG_fSFdjRr05G_JFb18XE83z1Rm

# Assignment 1
"""

s = "Hi there Class!"
print(s.split())

planet = "Earth"
diameter = 12742

print("The diameter of {} is {} kilometers".format(planet, diameter))

list1 =  [1,2,[3,4],[5,[100,200,['hello']],23,11],1,7]
print(list1[3][1][2][0])

d = { 'k1': [1,2,3, {'tricky':['oh','man','inception', {'target':[1,2,3, 'hello']}]}]}
print(d['k1'][3]['tricky'][3]['target'][3])

def generate_email(email):
     return email.split('@')[-1]

print(generate_email(input('email: ')))

def contained_in(a, b):
    return a in b
print(contained_in(input('String: '), input('Target: ')))

str1 = "Hello nname my name is name is name"
target = "name"

print(str1.count('name'))

seq = ['soup','dog','salad','cat','great'] 
print(list(filter(lambda x:x[0] == 's', seq)))

def challan(speed, is_birthday= False):
    speeds = [60, 80]
    if is_birthday:
        for i in range(len(speeds)):
            speeds[i] += 5
    if (speed <= speeds[0]):
        return 'No Challan'
    elif (speed <= speeds[1]):
        return 'Small Challan'
    else:
        return 'Heavy Challan'

print(challan(100, True))

list1 = ["M", "na", "i", "She"]
list2 = ["y", "me", "s", "lly"]
result = [list1[i]+list2[i] for i in range(len(list1))]
print(result)

list1 = ["Hello ", "take "]
list2 = ["Dear", "Sir"]

result = []

for i in list1:
    result += [i + a for a in list2]

print(result)

list1 = [10, 20, [300, 400, [5000, 6000], 500], 30, 40]
list1[2][2].append(7000)

print(list1)

list1 = [5, 20, 15, 20, 25, 50, 20]

for i in range(list1.count(20)):
    list1.remove(20)

print(list1)

d1 = {'a': 100, 'b': 200, 'c': 300}
print(200 in d1.values())

def do_sum(n: int):
    s = 0
    k = '2'
    for _ in range(n):
        s += int(k)
        k += '2'
    return s

print(do_sum(1))

"""# Assignment 2"""

import numpy as np
arr = np.array(10 * [5])
print(arr)

import numpy as np
arr = np.arange(10, 51)
print(arr)

import numpy as np
arr = np.arange(10, 51, 2)
print(arr)

import numpy as np
arr = np.arange(0,9).reshape(3, 3)
print(arr)

import numpy as np
arr = np.random.normal(0, 1, 25)
print(arr)

import numpy as np
arr = np.arange(0, 101) / 100
print(arr)

import numpy as np
arr = np.linspace(0, 1, 20)
print(arr)

import numpy as np
arr = np.arange(1, 26).reshape(5, 5)

print(arr)

print(arr[2:, 1:])

print(arr[0:3, 1:2])

print(arr.sum())

for i in range(5):
    print(f'Row {i+1}th sum -> ', arr[i].sum())
    print(f'Col {i+1}th sum -> ', arr[:, i].sum())

"""# Assignment 3"""

import numpy as np
import pandas as pd

data = pd.read_csv('./3/AWCustomers.csv')
data.head()

data.info()

"""## Feature Selection, Cleaning"""

unrelated_cols = [
    'CustomerID',
    'FirstName',
    'LastName',
    'MiddleName',
    'Suffix',
    'PhoneNumber',
    'LastUpdated'
]
data.drop(columns=unrelated_cols, inplace=True, errors='ignore')
data.head()

data.dropna(inplace=True, axis=1, thresh=1800) # 10%
data.dropna(inplace=True, axis=0)
data.head()

for column in data:
    print(column.ljust(25), str(len(data[column].unique())).rjust(10))

# Too much Address line 1, city, StateProvinceName, PostalCode
# dropping them
high_count_columns = [
    'AddressLine1',
    'City',
    'StateProvinceName',
    'PostalCode'
]
data.drop(columns=high_count_columns, inplace=True, errors='ignore')
data.head()

data.drop_duplicates(inplace=True)

"""## Data Preprocessing

"""

# Column                Attribute

# CountryRegionName     Nominal
# BirthDate             Interval
# Education             Ordinal
# Occupation            Ordinal
# Gender                Binary
# MaritalStatus         Binary
# HomeOwnerFlag         Binary
# NumberCarsOwned       Numeric
# NumberChildrenAtHome  Numeric
# TotalChildren         Numeric
# YearlyIncome          Numeric

nominal_attributes = ['CountryRegionName']
ordinal_attributes = ['Education', 'Occupation']
binary_named_attributes = ['Gender', 'MaritalStatus']

# Converting DOB to Age instead
from datetime import datetime

data['BirthDate'] = pd.to_datetime(data['BirthDate'])

CURRENT_TIME = datetime.now()
data['Age'] = data['BirthDate'].apply(
    lambda x: ((CURRENT_TIME - x).days // 365))

data.drop(columns=['BirthDate'], inplace=True)
data.head()

data.info()

for col in ordinal_attributes:
    print(data[col].value_counts(), '\n')

def column_mapper(data, columns=[], starts_zero=False):
    list_to_dict = lambda lst, starts_zero = False: {lst[i]:i if starts_zero else i+1 for i in range(len(lst))}

    mapping = []

    for col in columns:
        mapping.append(list_to_dict(data[col].unique()))

    for col, map_dict in zip(columns, mapping):
        print(map_dict)
        data[col] = data[col].map(map_dict)

column_mapper(data, ordinal_attributes)
column_mapper(data, binary_named_attributes, True)
column_mapper(data, nominal_attributes)
data.head()

# No more null values
data.isnull().sum()

# Normalize Continuous values like Income and Age
from sklearn.preprocessing import MinMaxScaler

scaling_cols = ['YearlyIncome', 'Age']
data[scaling_cols] = MinMaxScaler().fit_transform(data[scaling_cols])

from scipy.spatial import distance

display(data.corr())

display(distance.cosine(data['Education'], data['YearlyIncome']))
display(distance.jaccard(data['Education'], data['YearlyIncome']))

distance.euclidean(data['Education'], data['YearlyIncome'])

"""# Assignment 4"""

import numpy as np
import pandas as pd

# Covariance Matrix
# Eigen Values, Eigen Vectors
# sort Eigen Values in descending order
# matrix n eigen vectors
# matrix @ multiply -> Reduced Matrix

from sklearn.datasets import load_digits
digits = load_digits()

df = pd.DataFrame(data=digits.data)
df.head()

covariance_matrix = np.cov(df.T)

eigen_values, eigen_vectors = np.linalg.eigh(covariance_matrix)

sorted_index = np.argsort(eigen_values)[::-1]
sorted_eigenvalue = eigen_values[sorted_index]
sorted_eigenvectors = eigen_vectors[:,sorted_index]

sorted_eigenvalue

percent_eigen = sorted_eigenvalue / sorted_eigenvalue.sum()

sm = 0
i = 0
while sm < 0.95:
    sm += percent_eigen[i]
    i+=1

i

required_eigen_vectors = eigen_vectors[:,:i]
new_digits = digits.data @ required_eigen_vectors
print(new_digits.shape)
new_digits

"""# Assignment 5"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Y = X @ C
# C = inv(X' X) X' Y

from numpy.linalg import inv
def linear_regression(X, Y):
    """
    p => num of cases
    m => num of independent vars
    
    x -> (p, m)
    y -> (p, 1)
    """
    x = np.concatenate([np.ones([X.shape[0], 1]), X], axis=1)
    y = Y

    # y = xc
    # (p,1) = (p, m) (m, 1)
    
    # c = inv(x' x) x' y

    coeff_matrix = inv(x.T @ x) @ (x.T @ y)
    return coeff_matrix

# y = mx + c
data = {
    'X': [1400, 1600, 1700, 1875, 1100, 1550, 2350, 2450, 1425, 1700],
    'Y': [245, 312, 279, 308, 199, 219, 405, 324, 319, 255]
}

df = pd.DataFrame(data)
plt.scatter(data['X'], data['Y'])

coefficients = linear_regression(df.X.values.reshape(-1, 1),
                                 df.Y.values.reshape(-1, 1))

for x in [3000, 2000, 1500]:
    display(np.array([1, x]).T @ coefficients)

# Step 1
# Get X, Y matrix add 1 to start of each row of X

# Step 2
# Create a temporary coefficient matrix such that Y = XC

# Step 3
# C = inv(X' X) X' Y

from numpy.linalg import inv

data = {
    'Interest_Rate': [2.75] + 6*[2.5] + 3*[2.25] + 3*[2] + 11*[1.75],
    'Unemployement_Rate': 4*[5.3] + [5.4, 5.6] + 3*[5.5] +
        [5.6, 5.7, 5.9, 6, 5.9, 5.8, 6.1, 6.2] + 3*[6.1] + [5.9, 6.2, 6.2, 6.1],
    'Stock_Index_Price': [1464, 1394, 1357, 1293, 1256, 1254, 1234, 1195, 1159,
                        1167, 1130, 1075, 1047, 965, 943, 958, 971, 949, 884,
                        866, 876, 822, 704, 719],
}

df = pd.DataFrame(data)

independent_cols = df.columns[:-1]
dependent_col = df.columns[-1]

x = df[independent_cols].values
y = df[dependent_col].values.reshape(-1, 1)

coefficients = linear_regression(x, y)

test = np.array([[1, 1.5, 5.8]])

test @ coefficients

from sklearn.linear_model import LinearRegression

rg = LinearRegression()
rg.fit(x, y)
rg.predict([[1.5, 5.8]])

"""# Assignment 6"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

def hypothesis(theta, X):
    return 1 / (1 + np.exp(-(theta @ X.T)))) - 0.0000001

def cost(x, y, theta):
    y_predicted = hypothesis(x, theta)
    return np.sum(y*np.log(y_predicted) + (1-y)*np.log(1-y_predicted)) / -len(x)

url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'
df = pd.read_csv(url, names=['sepal length', 'sepal width', 'petal length', 'petal width', 'class'])
df.head()

mappings = {}

def preprocess(df):
    for col in df.columns:
        if df[col].dtype != float:
            uniq = df[col].unique()
            mappings[col] = {uniq[i]: i+1 for i in range(len(uniq))}
            df[col] = df[col].map(mappings[col])

preprocess(df)
df.head()

x = df.iloc[:,:-1]
y = df.iloc[:,-1]

extra_ones = pd.Series(1, index=df.index)
x = pd.concat([extra_ones, x], axis=1)

y1 = pd.concat(3*[y], axis=1)
for i in range(y1.shape[1]):
    temp = y1.iloc[:,i]
    y1.iloc[:,i] = temp.where(temp == i+1, 0)
    y1.iloc[:,i] //= i+1

#################################################################################
def gradient_descent(x, y, theta, alpha, iterations):
    m = len(x)
    for _ in range(iterations):
        for j in range(y.shape[1]):
            theta = pd.DataFrame(theta)
            h = hypothesis(theta.iloc[:,j], x)
            for k in range(theta.shape[0]):
                theta.iloc[k, j] -= alpha * np.sum((h-y.iloc[:, j])*x.iloc[:, k])
            theta = pd.DataFrame(theta)
    return theta

theta = np.zeros([x.shape[1], y1.shape[1]])
theta = gradient_descent(x, y1, theta, 0.02, 500)
theta

output = []
for i in range(len(y.unique())):
    theta1 = pd.DataFrame(theta)
    h = hypothesis(theta1.iloc[:, i], x)
    output.append(h)
output = pd.DataFrame(np.array(output).T)

# output = output.idxmax(axis=1)
output = output.idxmax(axis=1).map({v-1:k for k, v in mappings['class'].items()})
# pd.concat([output, df.iloc[:,-1]], axis=1)
output.head()

from sklearn.datasets import load_iris

iris = load_iris()
x = iris.data
y = iris.target

x

from sklearn.linear_model import LogisticRegression

lr = LogisticRegression(multi_class='ovr')
lr.fit(x, y)
lr.predict([[5.10, 2.8 , 6, 1.8]])

"""# Assignment 7"""

import numpy as np
import pandas as pd

from sklearn.datasets import load_iris

iris = load_iris()
x = np.array(iris.data)
y = np.array(iris.target)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(x)

x = scaler.transform(x)

from sklearn.naive_bayes import GaussianNB

gaussian = GaussianNB()
gaussian.fit(x, y)
predicted = gaussian.predict(x)
predicted

from sklearn.metrics import classification_report, confusion_matrix
print(classification_report(y, predicted))
print(confusion_matrix(y, predicted))

"""# Assignment 8"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.datasets import load_iris

iris = load_iris()
x = iris.data
y = iris.target

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(x_train)
x_train_scaled = scaler.transform(x_train)
x_test_scaled = scaler.transform(x_test)

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=2)
knn.fit(x_train_scaled, y_train)
predicted = knn.predict(x_test_scaled)

from sklearn.metrics import mean_squared_error
print(predicted)
print(y_test)

print(mean_squared_error(y_test, predicted))

k_range = range(1, 21)

errors = []

for i in k_range:
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(x_train_scaled, y_train)
    predicted = knn.predict(x_test_scaled)
    errors.append(mean_squared_error(y_test, predicted))

plt.plot(k_range, errors, marker='o')
# Means k = 1 is best !!
# Can Only do for one data set, how many can you do ?

from sklearn.model_selection import GridSearchCV

param_grid = {'n_neighbors': k_range}
grid = GridSearchCV(KNeighborsClassifier(), param_grid=param_grid, scoring='accuracy', cv=10)
grid.fit(x_train_scaled, y_train)
plt.plot(k_range, grid.cv_results_['mean_test_score'], marker='o')

# Does by splitting !!